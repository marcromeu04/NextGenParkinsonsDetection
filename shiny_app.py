{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "36871ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shiny_app.py\n",
    "from shiny import App, ui, reactive\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Cargar al paciente\n",
    "with open(\"paciente_347.pkl\", 'rb') as archivo:\n",
    "            paciente = pickle.load(archivo)\n",
    "\n",
    "# Cargar la base de datos\n",
    "with open(\"DatosCompletos.pkl\", 'rb') as archivo:\n",
    "            base_datos = pickle.load(archivo)\n",
    "\n",
    "# Cargar boss\n",
    "with open(\"boss_features.pkl\", 'rb') as archivo:\n",
    "            base_datos_boss = pickle.load(archivo)\n",
    "\n",
    "# Cargar esta\n",
    "with open(\"estadisticos_features.pkl\", 'rb') as archivo:\n",
    "            base_datos_esta = pickle.load(archivo)\n",
    "\n",
    "# Cargar psd\n",
    "base_datos_psd = joblib.load(\"psd_features.pkl\")\n",
    "\n",
    "\n",
    "# Cargar modelos y variables al iniciar\n",
    "meta_model = joblib.load(\"meta_model_xgb.pkl\")\n",
    "\n",
    "selected_vars_cues = joblib.load(\"selected_vars_cues.pkl\")\n",
    "selected_vars_boss = joblib.load(\"selected_vars_boss.pkl\")\n",
    "selected_vars_esta = joblib.load(\"selected_vars_esta.pkl\")\n",
    "selected_vars_psd = joblib.load(\"selected_vars_psd.pkl\")\n",
    "\n",
    "boss_models = joblib.load(\"boss_models.pkl\")\n",
    "\n",
    "# Cargar AutoML\n",
    "\n",
    "predictor_cues = TabularPredictor.load(\"AutogluonModels/ag-20250508_072135\")\n",
    "predictor_boss = TabularPredictor.load(\"AutogluonModels/ag-20250508_093716\")\n",
    "predictor_esta = TabularPredictor.load(\"AutogluonModels/ag-20250508_094129\")\n",
    "predictor_psd = TabularPredictor.load(\"AutogluonModels/ag-20250508_101709\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5009cf",
   "metadata": {},
   "source": [
    "Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d620bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paciente_df=pd.DataFrame(paciente).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1eaa15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = paciente_df\n",
    "df = df.drop(columns=[col for col in df.columns if col.endswith('_Time')])\n",
    "\n",
    "df_final=df.drop(columns=['resource_type', 'study_id', 'condition', 'disease_comment','age_at_diagnosis'])\n",
    "df = df.drop(columns=['id', 'resource_type', 'study_id', 'condition', 'disease_comment','age_at_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d5dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cuestionario=df.iloc[:, :38]\n",
    "df_series=df.iloc[:, 38:]\n",
    "df_final = df_final.iloc[:, [0] + list(range(39, df_final.shape[1]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76cbd2",
   "metadata": {},
   "source": [
    "CUESTIONARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93c356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age     02  height  weight    08    25    01     09    05  gender_male  \\\n",
      "347   67  False     180      83  True  True  True  False  True         True   \n",
      "\n",
      "     ...     03     22     21  effect_of_alcohol_on_tremor_Unknown     07  \\\n",
      "347  ...  False  False  False                                 True  False   \n",
      "\n",
      "        06  handedness_right     13     15     26  \n",
      "347  False              True  False  False  False  \n",
      "\n",
      "[1 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# filtramos las variables más importantes\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "categorical_columns = df_cuestionario.columns[3:8]\n",
    "# Variables categóricas y sus categorías conocidas\n",
    "categorical_levels = {\n",
    "    'gender': ['male', 'female'],\n",
    "    'handedness': ['left', 'right'],\n",
    "    'appearance_in_kinship': [True, False],\n",
    "    'appearance_in_first_grade_kinship': [True, False],\n",
    "    'effect_of_alcohol_on_tremor': ['Worsening', 'No effect', 'Unknown']\n",
    "}\n",
    "\n",
    "# Convertir a categorías con valores posibles definidos (aunque no aparezcan en este caso)\n",
    "for col, categories in categorical_levels.items():\n",
    "    df[col] = pd.Categorical(df[col], categories=categories)\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_levels.keys(), drop_first=False)\n",
    "X_cues = df_encoded.replace({'True': 1, 'False': 0})\n",
    "X_cues_filtered = X_cues[selected_vars_cues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9904a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = base_datos.columns[3:8]\n",
    "df_encoded = pd.get_dummies(base_datos, columns=categorical_levels.keys(), drop_first=False)\n",
    "X_cues = df_encoded.replace({'True': 1, 'False': 0})\n",
    "base_datos_cues = X_cues[selected_vars_cues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82dcfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_cues = predictor_cues.predict(X_cues_filtered)\n",
    "probabilidad_cues = predictor_cues.predict_proba(X_cues_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f52f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4e7c63eac840d7a86c0d40bb25d2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shap\n",
    "def predict_fn(X_array):\n",
    "    X_df = pd.DataFrame(X_array, columns=X_cues_filtered.columns)\n",
    "    return predictor_cues.predict_proba(X_df).values  \n",
    "\n",
    "background = base_datos_cues.sample(n=100, random_state=0)  \n",
    "\n",
    "# Crear el explainer con datos de fondo (background)\n",
    "explainer = shap.KernelExplainer(predict_fn, background)\n",
    "\n",
    "shap_values = explainer.shap_values(X_cues_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322e2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "n_samples = shap_values.shape[0]\n",
    "feature_names = X_cues_filtered.columns\n",
    "\n",
    "top_features_cues = []\n",
    "\n",
    "predicted_class = predictor_cues.predict(X_cues_filtered).values[0]\n",
    "shap_vector = shap_values[0, :, predicted_class]  # (n_features,)\n",
    "# Asociar cada SHAP value con su feature\n",
    "df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap_value': shap_vector,\n",
    "    'abs_shap': np.abs(shap_vector)\n",
    "})\n",
    "\n",
    "# Seleccionar top N por magnitud\n",
    "top_10 = df.sort_values('abs_shap', ascending=False).head(top_n).reset_index(drop=True)\n",
    "\n",
    "\n",
    "top_features_cues.append({\n",
    "    'patient_index': 0,\n",
    "    'predicted_class': predicted_class,\n",
    "    'top_features': top_10[['feature', 'shap_value']]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46bf8e",
   "metadata": {},
   "source": [
    "BOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07dff845",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_patient=df_series.loc[347]\n",
    "new_patient_boss = pd.DataFrame(new_patient).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.transformation import BOSS\n",
    "\n",
    "def apply_boss_to_series(series, boss_model):\n",
    "    # Aplicar la misma transformación BOSS previamente entrenada\n",
    "    transformed = boss_model.transform(series.reshape(1, -1)) \n",
    "    return transformed[0].toarray()   # Extraemos la primera fila transformada\n",
    "\n",
    "    \n",
    "def transform_single_individual(patient_dict, boss_models, window_sizes=[20, 40, 80]):\n",
    "    transformed_patient = {}\n",
    "\n",
    "    for col, signal in patient_dict.items():\n",
    "        if col == 'id':\n",
    "            transformed_patient['id'] = signal\n",
    "            continue\n",
    "\n",
    "        all_transformed = []\n",
    "\n",
    "        for w in window_sizes:\n",
    "            model = boss_models.get((col, w))\n",
    "            if model is None:\n",
    "                raise ValueError(f\"No BOSS model found for column {col} with window {w}\")\n",
    "\n",
    "            transformed = apply_boss_to_series(np.array(signal), model)\n",
    "            all_transformed.append(transformed)\n",
    "\n",
    "        concatenated = np.concatenate(all_transformed)\n",
    "        transformed_patient[col] = concatenated\n",
    "\n",
    "    return transformed_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d180109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_patient_boss=df_series.loc[347]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b22c0edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transform_single_individual(new_patient_boss, boss_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1dbafe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_columns = {}\n",
    "\n",
    "for col, vector in transformed.items():\n",
    "    vector = np.array(vector).flatten()  # Asegura forma plana\n",
    "    vec_length = vector.shape[0]\n",
    "\n",
    "    # Crear columnas nombradas como col_vec0, col_vec1, ...\n",
    "    for i in range(vec_length):\n",
    "        expanded_columns[f\"{col}_vec{i}\"] = vector[i]\n",
    "\n",
    "# 2. Crear un DataFrame con una sola fila\n",
    "expanded_df_boss = pd.DataFrame([expanded_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "36e0a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_boss_filtered=expanded_df_boss[selected_vars_boss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_boss = predictor_boss.predict(X_boss_filtered)\n",
    "probabilidad_boss = predictor_boss.predict_proba(X_boss_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5adf42bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15d7aa9c0584323a7d1754171fb0138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "base_datos_boss_filtered=base_datos_boss[selected_vars_boss]\n",
    "def predict_fn(X_array):\n",
    "    X_df = pd.DataFrame(X_array, columns=X_boss_filtered.columns)\n",
    "    return predictor_boss.predict_proba(X_df).values  \n",
    "\n",
    "background = base_datos_boss_filtered.sample(n=100, random_state=0)  \n",
    "\n",
    "# Crear el explainer con datos de fondo (background)\n",
    "explainer = shap.KernelExplainer(predict_fn, background)\n",
    "\n",
    "shap_values = explainer.shap_values(X_boss_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84398f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "n_samples = shap_values.shape[0]\n",
    "feature_names = X_boss_filtered.columns\n",
    "\n",
    "top_features_boss = []\n",
    "\n",
    "predicted_class = predictor_boss.predict(X_boss_filtered).values[0]\n",
    "shap_vector = shap_values[0, :, predicted_class]  # (n_features,)\n",
    "# Asociar cada SHAP value con su feature\n",
    "df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap_value': shap_vector,\n",
    "    'abs_shap': np.abs(shap_vector)\n",
    "})\n",
    "\n",
    "# Seleccionar top N por magnitud\n",
    "top_10 = df.sort_values('abs_shap', ascending=False).head(top_n).reset_index(drop=True)\n",
    "\n",
    "\n",
    "top_features_boss.append({\n",
    "    'patient_index': 0,\n",
    "    'predicted_class': predicted_class,\n",
    "    'top_features': top_10[['feature', 'shap_value']]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc8ace",
   "metadata": {},
   "source": [
    "ESTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee38f5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas de series temporales detectadas (antes de filtrar): ['Relaxed x LeftWrist_Accelerometer_X', 'Relaxed x LeftWrist_Accelerometer_Y', 'Relaxed x LeftWrist_Accelerometer_Z', 'Relaxed x LeftWrist_Gyroscope_X', 'Relaxed x LeftWrist_Gyroscope_Y', 'Relaxed x LeftWrist_Gyroscope_Z', 'Relaxed x RightWrist_Accelerometer_X', 'Relaxed x RightWrist_Accelerometer_Y', 'Relaxed x RightWrist_Accelerometer_Z', 'Relaxed x RightWrist_Gyroscope_X', 'Relaxed x RightWrist_Gyroscope_Y', 'Relaxed x RightWrist_Gyroscope_Z', 'RelaxedTask x LeftWrist_Accelerometer_X', 'RelaxedTask x LeftWrist_Accelerometer_Y', 'RelaxedTask x LeftWrist_Accelerometer_Z', 'RelaxedTask x LeftWrist_Gyroscope_X', 'RelaxedTask x LeftWrist_Gyroscope_Y', 'RelaxedTask x LeftWrist_Gyroscope_Z', 'RelaxedTask x RightWrist_Accelerometer_X', 'RelaxedTask x RightWrist_Accelerometer_Y', 'RelaxedTask x RightWrist_Accelerometer_Z', 'RelaxedTask x RightWrist_Gyroscope_X', 'RelaxedTask x RightWrist_Gyroscope_Y', 'RelaxedTask x RightWrist_Gyroscope_Z', 'StretchHold x LeftWrist_Accelerometer_X', 'StretchHold x LeftWrist_Accelerometer_Y', 'StretchHold x LeftWrist_Accelerometer_Z', 'StretchHold x LeftWrist_Gyroscope_X', 'StretchHold x LeftWrist_Gyroscope_Y', 'StretchHold x LeftWrist_Gyroscope_Z', 'StretchHold x RightWrist_Accelerometer_X', 'StretchHold x RightWrist_Accelerometer_Y', 'StretchHold x RightWrist_Accelerometer_Z', 'StretchHold x RightWrist_Gyroscope_X', 'StretchHold x RightWrist_Gyroscope_Y', 'StretchHold x RightWrist_Gyroscope_Z', 'LiftHold x LeftWrist_Accelerometer_X', 'LiftHold x LeftWrist_Accelerometer_Y', 'LiftHold x LeftWrist_Accelerometer_Z', 'LiftHold x LeftWrist_Gyroscope_X', 'LiftHold x LeftWrist_Gyroscope_Y', 'LiftHold x LeftWrist_Gyroscope_Z', 'LiftHold x RightWrist_Accelerometer_X', 'LiftHold x RightWrist_Accelerometer_Y', 'LiftHold x RightWrist_Accelerometer_Z', 'LiftHold x RightWrist_Gyroscope_X', 'LiftHold x RightWrist_Gyroscope_Y', 'LiftHold x RightWrist_Gyroscope_Z', 'HoldWeight x LeftWrist_Accelerometer_X', 'HoldWeight x LeftWrist_Accelerometer_Y', 'HoldWeight x LeftWrist_Accelerometer_Z', 'HoldWeight x LeftWrist_Gyroscope_X', 'HoldWeight x LeftWrist_Gyroscope_Y', 'HoldWeight x LeftWrist_Gyroscope_Z', 'HoldWeight x RightWrist_Accelerometer_X', 'HoldWeight x RightWrist_Accelerometer_Y', 'HoldWeight x RightWrist_Accelerometer_Z', 'HoldWeight x RightWrist_Gyroscope_X', 'HoldWeight x RightWrist_Gyroscope_Y', 'HoldWeight x RightWrist_Gyroscope_Z', 'PointFinger x LeftWrist_Accelerometer_X', 'PointFinger x LeftWrist_Accelerometer_Y', 'PointFinger x LeftWrist_Accelerometer_Z', 'PointFinger x LeftWrist_Gyroscope_X', 'PointFinger x LeftWrist_Gyroscope_Y', 'PointFinger x LeftWrist_Gyroscope_Z', 'PointFinger x RightWrist_Accelerometer_X', 'PointFinger x RightWrist_Accelerometer_Y', 'PointFinger x RightWrist_Accelerometer_Z', 'PointFinger x RightWrist_Gyroscope_X', 'PointFinger x RightWrist_Gyroscope_Y', 'PointFinger x RightWrist_Gyroscope_Z', 'DrinkGlas x LeftWrist_Accelerometer_X', 'DrinkGlas x LeftWrist_Accelerometer_Y', 'DrinkGlas x LeftWrist_Accelerometer_Z', 'DrinkGlas x LeftWrist_Gyroscope_X', 'DrinkGlas x LeftWrist_Gyroscope_Y', 'DrinkGlas x LeftWrist_Gyroscope_Z', 'DrinkGlas x RightWrist_Accelerometer_X', 'DrinkGlas x RightWrist_Accelerometer_Y', 'DrinkGlas x RightWrist_Accelerometer_Z', 'DrinkGlas x RightWrist_Gyroscope_X', 'DrinkGlas x RightWrist_Gyroscope_Y', 'DrinkGlas x RightWrist_Gyroscope_Z', 'CrossArms x LeftWrist_Accelerometer_X', 'CrossArms x LeftWrist_Accelerometer_Y', 'CrossArms x LeftWrist_Accelerometer_Z', 'CrossArms x LeftWrist_Gyroscope_X', 'CrossArms x LeftWrist_Gyroscope_Y', 'CrossArms x LeftWrist_Gyroscope_Z', 'CrossArms x RightWrist_Accelerometer_X', 'CrossArms x RightWrist_Accelerometer_Y', 'CrossArms x RightWrist_Accelerometer_Z', 'CrossArms x RightWrist_Gyroscope_X', 'CrossArms x RightWrist_Gyroscope_Y', 'CrossArms x RightWrist_Gyroscope_Z', 'TouchIndex x LeftWrist_Accelerometer_X', 'TouchIndex x LeftWrist_Accelerometer_Y', 'TouchIndex x LeftWrist_Accelerometer_Z', 'TouchIndex x LeftWrist_Gyroscope_X', 'TouchIndex x LeftWrist_Gyroscope_Y', 'TouchIndex x LeftWrist_Gyroscope_Z', 'TouchIndex x RightWrist_Accelerometer_X', 'TouchIndex x RightWrist_Accelerometer_Y', 'TouchIndex x RightWrist_Accelerometer_Z', 'TouchIndex x RightWrist_Gyroscope_X', 'TouchIndex x RightWrist_Gyroscope_Y', 'TouchIndex x RightWrist_Gyroscope_Z', 'TouchNose x LeftWrist_Accelerometer_X', 'TouchNose x LeftWrist_Accelerometer_Y', 'TouchNose x LeftWrist_Accelerometer_Z', 'TouchNose x LeftWrist_Gyroscope_X', 'TouchNose x LeftWrist_Gyroscope_Y', 'TouchNose x LeftWrist_Gyroscope_Z', 'TouchNose x RightWrist_Accelerometer_X', 'TouchNose x RightWrist_Accelerometer_Y', 'TouchNose x RightWrist_Accelerometer_Z', 'TouchNose x RightWrist_Gyroscope_X', 'TouchNose x RightWrist_Gyroscope_Y', 'TouchNose x RightWrist_Gyroscope_Z', 'Entrainment x LeftWrist_Accelerometer_X', 'Entrainment x LeftWrist_Accelerometer_Y', 'Entrainment x LeftWrist_Accelerometer_Z', 'Entrainment x LeftWrist_Gyroscope_X', 'Entrainment x LeftWrist_Gyroscope_Y', 'Entrainment x LeftWrist_Gyroscope_Z', 'Entrainment x RightWrist_Accelerometer_X', 'Entrainment x RightWrist_Accelerometer_Y', 'Entrainment x RightWrist_Accelerometer_Z', 'Entrainment x RightWrist_Gyroscope_X', 'Entrainment x RightWrist_Gyroscope_Y', 'Entrainment x RightWrist_Gyroscope_Z']\n",
      "Procesando las primeras 132 series: ['Relaxed x LeftWrist_Accelerometer_X', 'Relaxed x LeftWrist_Accelerometer_Y', 'Relaxed x LeftWrist_Accelerometer_Z', 'Relaxed x LeftWrist_Gyroscope_X', 'Relaxed x LeftWrist_Gyroscope_Y', 'Relaxed x LeftWrist_Gyroscope_Z', 'Relaxed x RightWrist_Accelerometer_X', 'Relaxed x RightWrist_Accelerometer_Y', 'Relaxed x RightWrist_Accelerometer_Z', 'Relaxed x RightWrist_Gyroscope_X', 'Relaxed x RightWrist_Gyroscope_Y', 'Relaxed x RightWrist_Gyroscope_Z', 'RelaxedTask x LeftWrist_Accelerometer_X', 'RelaxedTask x LeftWrist_Accelerometer_Y', 'RelaxedTask x LeftWrist_Accelerometer_Z', 'RelaxedTask x LeftWrist_Gyroscope_X', 'RelaxedTask x LeftWrist_Gyroscope_Y', 'RelaxedTask x LeftWrist_Gyroscope_Z', 'RelaxedTask x RightWrist_Accelerometer_X', 'RelaxedTask x RightWrist_Accelerometer_Y', 'RelaxedTask x RightWrist_Accelerometer_Z', 'RelaxedTask x RightWrist_Gyroscope_X', 'RelaxedTask x RightWrist_Gyroscope_Y', 'RelaxedTask x RightWrist_Gyroscope_Z', 'StretchHold x LeftWrist_Accelerometer_X', 'StretchHold x LeftWrist_Accelerometer_Y', 'StretchHold x LeftWrist_Accelerometer_Z', 'StretchHold x LeftWrist_Gyroscope_X', 'StretchHold x LeftWrist_Gyroscope_Y', 'StretchHold x LeftWrist_Gyroscope_Z', 'StretchHold x RightWrist_Accelerometer_X', 'StretchHold x RightWrist_Accelerometer_Y', 'StretchHold x RightWrist_Accelerometer_Z', 'StretchHold x RightWrist_Gyroscope_X', 'StretchHold x RightWrist_Gyroscope_Y', 'StretchHold x RightWrist_Gyroscope_Z', 'LiftHold x LeftWrist_Accelerometer_X', 'LiftHold x LeftWrist_Accelerometer_Y', 'LiftHold x LeftWrist_Accelerometer_Z', 'LiftHold x LeftWrist_Gyroscope_X', 'LiftHold x LeftWrist_Gyroscope_Y', 'LiftHold x LeftWrist_Gyroscope_Z', 'LiftHold x RightWrist_Accelerometer_X', 'LiftHold x RightWrist_Accelerometer_Y', 'LiftHold x RightWrist_Accelerometer_Z', 'LiftHold x RightWrist_Gyroscope_X', 'LiftHold x RightWrist_Gyroscope_Y', 'LiftHold x RightWrist_Gyroscope_Z', 'HoldWeight x LeftWrist_Accelerometer_X', 'HoldWeight x LeftWrist_Accelerometer_Y', 'HoldWeight x LeftWrist_Accelerometer_Z', 'HoldWeight x LeftWrist_Gyroscope_X', 'HoldWeight x LeftWrist_Gyroscope_Y', 'HoldWeight x LeftWrist_Gyroscope_Z', 'HoldWeight x RightWrist_Accelerometer_X', 'HoldWeight x RightWrist_Accelerometer_Y', 'HoldWeight x RightWrist_Accelerometer_Z', 'HoldWeight x RightWrist_Gyroscope_X', 'HoldWeight x RightWrist_Gyroscope_Y', 'HoldWeight x RightWrist_Gyroscope_Z', 'PointFinger x LeftWrist_Accelerometer_X', 'PointFinger x LeftWrist_Accelerometer_Y', 'PointFinger x LeftWrist_Accelerometer_Z', 'PointFinger x LeftWrist_Gyroscope_X', 'PointFinger x LeftWrist_Gyroscope_Y', 'PointFinger x LeftWrist_Gyroscope_Z', 'PointFinger x RightWrist_Accelerometer_X', 'PointFinger x RightWrist_Accelerometer_Y', 'PointFinger x RightWrist_Accelerometer_Z', 'PointFinger x RightWrist_Gyroscope_X', 'PointFinger x RightWrist_Gyroscope_Y', 'PointFinger x RightWrist_Gyroscope_Z', 'DrinkGlas x LeftWrist_Accelerometer_X', 'DrinkGlas x LeftWrist_Accelerometer_Y', 'DrinkGlas x LeftWrist_Accelerometer_Z', 'DrinkGlas x LeftWrist_Gyroscope_X', 'DrinkGlas x LeftWrist_Gyroscope_Y', 'DrinkGlas x LeftWrist_Gyroscope_Z', 'DrinkGlas x RightWrist_Accelerometer_X', 'DrinkGlas x RightWrist_Accelerometer_Y', 'DrinkGlas x RightWrist_Accelerometer_Z', 'DrinkGlas x RightWrist_Gyroscope_X', 'DrinkGlas x RightWrist_Gyroscope_Y', 'DrinkGlas x RightWrist_Gyroscope_Z', 'CrossArms x LeftWrist_Accelerometer_X', 'CrossArms x LeftWrist_Accelerometer_Y', 'CrossArms x LeftWrist_Accelerometer_Z', 'CrossArms x LeftWrist_Gyroscope_X', 'CrossArms x LeftWrist_Gyroscope_Y', 'CrossArms x LeftWrist_Gyroscope_Z', 'CrossArms x RightWrist_Accelerometer_X', 'CrossArms x RightWrist_Accelerometer_Y', 'CrossArms x RightWrist_Accelerometer_Z', 'CrossArms x RightWrist_Gyroscope_X', 'CrossArms x RightWrist_Gyroscope_Y', 'CrossArms x RightWrist_Gyroscope_Z', 'TouchIndex x LeftWrist_Accelerometer_X', 'TouchIndex x LeftWrist_Accelerometer_Y', 'TouchIndex x LeftWrist_Accelerometer_Z', 'TouchIndex x LeftWrist_Gyroscope_X', 'TouchIndex x LeftWrist_Gyroscope_Y', 'TouchIndex x LeftWrist_Gyroscope_Z', 'TouchIndex x RightWrist_Accelerometer_X', 'TouchIndex x RightWrist_Accelerometer_Y', 'TouchIndex x RightWrist_Accelerometer_Z', 'TouchIndex x RightWrist_Gyroscope_X', 'TouchIndex x RightWrist_Gyroscope_Y', 'TouchIndex x RightWrist_Gyroscope_Z', 'TouchNose x LeftWrist_Accelerometer_X', 'TouchNose x LeftWrist_Accelerometer_Y', 'TouchNose x LeftWrist_Accelerometer_Z', 'TouchNose x LeftWrist_Gyroscope_X', 'TouchNose x LeftWrist_Gyroscope_Y', 'TouchNose x LeftWrist_Gyroscope_Z', 'TouchNose x RightWrist_Accelerometer_X', 'TouchNose x RightWrist_Accelerometer_Y', 'TouchNose x RightWrist_Accelerometer_Z', 'TouchNose x RightWrist_Gyroscope_X', 'TouchNose x RightWrist_Gyroscope_Y', 'TouchNose x RightWrist_Gyroscope_Z', 'Entrainment x LeftWrist_Accelerometer_X', 'Entrainment x LeftWrist_Accelerometer_Y', 'Entrainment x LeftWrist_Accelerometer_Z', 'Entrainment x LeftWrist_Gyroscope_X', 'Entrainment x LeftWrist_Gyroscope_Y', 'Entrainment x LeftWrist_Gyroscope_Z', 'Entrainment x RightWrist_Accelerometer_X', 'Entrainment x RightWrist_Accelerometer_Y', 'Entrainment x RightWrist_Accelerometer_Z', 'Entrainment x RightWrist_Gyroscope_X', 'Entrainment x RightWrist_Gyroscope_Y', 'Entrainment x RightWrist_Gyroscope_Z']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Extraction:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 22.11it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 23.45it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 22.20it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 22.62it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 28.92it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 31.69it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 18.51it/s]\n",
      "Feature Extraction:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 10.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 25.62it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 29.63it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 30.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 21.48it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 30.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 24.14it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 23.74it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 21.82it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 26.04it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 28.50it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 24.53it/s]\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 22.35it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 24.27it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 36.54it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 36.37it/s]\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 23.99it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 27.91it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 30.74it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 19.56it/s]\n",
      "Feature Extraction:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 32.65it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 29.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 13.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 26.13it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 30.03it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 36.13it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 28.94it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 29.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 56.80it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 29.40it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 37.19it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 50.06it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 30.66it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 30.80it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 17.57it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 19.78it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 17.26it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 37.48it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 32.91it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 33.13it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 16.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 16.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 14.28it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 26.17it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 36.30it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 30.82it/s]\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 35.05it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 24.76it/s]\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 42.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 33.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 35.06it/s]\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 28.71it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 34.58it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 34.02it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 36.47it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 33.92it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 35.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 36.77it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 15.15it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 16.88it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 14.67it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 14.76it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 13.97it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 14.61it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 25.76it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "Feature Extraction:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 39.82it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 39.70it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 30.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 30.90it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 33.18it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 36.29it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 33.28it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 66.67it/s]\n",
      "Feature Extraction:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 61.44it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 44.20it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 53.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 36.97it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 33.89it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 34.56it/s]\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 47.15it/s]\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 34.32it/s]\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 15.72it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 15.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 15.84it/s]\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 28.25it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 36.77it/s]\n",
      "Feature Extraction:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 37.58it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 45.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 34.10it/s]\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 41.05it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 37.69it/s]\n",
      "Feature Extraction:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 35.51it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 44.20it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 35.25it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 28.74it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 28.35it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 42.77it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 40.64it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 34.16it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 16.45it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 13.94it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 16.55it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 14.22it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 15.22it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 27.89it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 25.50it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 38.29it/s]\n",
      "Feature Extraction:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 36.05it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 29.68it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 30.64it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 29.93it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 31.66it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 21.47it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 24.08it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 24.75it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 20.16it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 14.87it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 12.03it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 14.24it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 16.12it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 13.63it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 14.78it/s]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:00<00:00, 50.44it/s]\n"
     ]
    }
   ],
   "source": [
    "new_patient_esta=df_final.loc[347]\n",
    "df_final = pd.DataFrame(new_patient_esta).T\n",
    "#estadisticos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import traceback\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "# ----- PARTE 1: Extracción de features con tsfresh -----\n",
    "\n",
    "# Número de series temporales (columnas) a procesar (ajusta según lo que necesites)\n",
    "num_series = 132\n",
    "\n",
    "# Obtener el número de núcleos disponibles\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Diccionario de parámetros para extraer features con tsfresh\n",
    "custom_fc_parameters = {\n",
    "    \"absolute_sum_of_changes\": None,\n",
    "    \"mean\": None,\n",
    "    \"median\": None,\n",
    "    \"standard_deviation\": None,\n",
    "    \"variance\": None,\n",
    "    \"skewness\": None,\n",
    "    \"kurtosis\": None,\n",
    "    \"maximum\": None,\n",
    "    \"absolute_maximum\": None,\n",
    "    \"first_location_of_maximum\": None,\n",
    "    \"first_location_of_minimum\": None,\n",
    "    \"last_location_of_maximum\": None,\n",
    "    \"last_location_of_minimum\": None,\n",
    "    \"sum_values\": None,\n",
    "    \"sum_of_reoccurring_data_points\": None,\n",
    "    \"range_count\": [{\"min\": 0.5, \"max\": 2.0}],\n",
    "    \"linear_trend\": [\n",
    "        {\"attr\": \"slope\"},\n",
    "        {\"attr\": \"intercept\"},\n",
    "        {\"attr\": \"rvalue\"},\n",
    "        {\"attr\": \"pvalue\"}\n",
    "    ],\n",
    "    \"fft_coefficient\": [\n",
    "        {\"coeff\": 0, \"attr\": \"real\"},\n",
    "        {\"coeff\": 0, \"attr\": \"imag\"},\n",
    "        {\"coeff\": 1, \"attr\": \"real\"},\n",
    "        {\"coeff\": 1, \"attr\": \"imag\"}\n",
    "    ],\n",
    "    \"quantile\": [{\"q\": 0.25}, {\"q\": 0.5}, {\"q\": 0.75}],\n",
    "    # En lugar de \"q_1\" y \"q_2\", se pasa \"q\" como tupla\n",
    "    \"autocorrelation\": [{\"lag\": 1}, {\"lag\": 2}],\n",
    "    # Se define cid_ce con el parámetro requerido \"normalize\"\n",
    "    \"cid_ce\": [{\"normalize\": True}],\n",
    "    \"binned_entropy\": [{\"max_bins\": 10}],\n",
    "    \"index_mass_quantile\": [{\"q\": 0.5}],\n",
    "    \"partial_autocorrelation\": [{\"lag\": 1}, {\"lag\": 2}],\n",
    "    # Se le suministra un parámetro \"r\" para symmetry_looking\n",
    "    \"symmetry_looking\": [{\"r\": 0.5}],\n",
    "    \"longest_strike_above_mean\": None,\n",
    "    \"longest_strike_below_mean\": None,\n",
    "    # Suministramos el argumento requerido \"n\" para number_peaks\n",
    "    \"number_peaks\": [{\"n\": 3}]}\n",
    "\n",
    "def process_series_column(args):\n",
    "    \"\"\"\n",
    "    Extrae las features de una columna de series temporales usando tsfresh.\n",
    "    Recibe una tupla (col, df_col) donde:\n",
    "      - col: nombre de la columna.\n",
    "      - df_col: DataFrame con solo las columnas \"id\" y la columna de interés.\n",
    "    \"\"\"\n",
    "    col, df_col = args\n",
    "    try:\n",
    "        long_df_list = []\n",
    "        # Convertir cada serie de la columna a formato \"long\"\n",
    "        for _, row in df_col.iterrows():\n",
    "            ts = row[col]\n",
    "            if isinstance(ts, list) and len(ts) > 0:\n",
    "                df_long = pd.DataFrame({\n",
    "                    'id': row['id'],\n",
    "                    'time': np.arange(len(ts)),\n",
    "                    'value': ts\n",
    "                })\n",
    "                long_df_list.append(df_long)\n",
    "        if long_df_list:\n",
    "            df_long_col = pd.concat(long_df_list, ignore_index=True)\n",
    "            extracted_features = extract_features(\n",
    "                df_long_col,\n",
    "                column_id=\"id\",\n",
    "                column_sort=\"time\",\n",
    "                column_value=\"value\",\n",
    "                default_fc_parameters=custom_fc_parameters,\n",
    "                impute_function=impute,\n",
    "                n_jobs=1\n",
    "            )\n",
    "            extracted_features = extracted_features.add_prefix(f\"{col}_\")\n",
    "            return extracted_features\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando la columna {col}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "time_series_cols = [col for col in df_final.columns if df_final[col].apply(lambda x: isinstance(x, list)).any()]\n",
    "print(\"Columnas de series temporales detectadas (antes de filtrar):\", time_series_cols)\n",
    "\n",
    "time_series_cols = time_series_cols[:num_series]\n",
    "print(f\"Procesando las primeras {num_series} series: {time_series_cols}\")\n",
    "\n",
    "args_list = []\n",
    "for col in time_series_cols:\n",
    "    df_col = df_final[['id', col]].copy()\n",
    "    args_list.append((col, df_col))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=n_cores) as executor:\n",
    "    results = list(executor.map(process_series_column, args_list))\n",
    "\n",
    "features_list = [res for res in results if res is not None]\n",
    "\n",
    "if features_list:\n",
    "    ts_features = pd.concat(features_list, axis=1)\n",
    "    df_final_features = df_final.merge(ts_features, left_on='id', right_index=True, how='left')\n",
    "    df_final_features = df_final_features.drop(columns=time_series_cols)\n",
    "else:\n",
    "    df_final_features = df_final.copy()\n",
    "\n",
    "df_patient=df_final_features.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b89565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_esta_filtered = df_patient[selected_vars_esta]\n",
    "prediccion_esta = predictor_esta.predict(X_esta_filtered)\n",
    "probabilidad_esta = predictor_esta.predict_proba(X_esta_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e778627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_datos_esta_filtered=base_datos_esta[selected_vars_esta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68ae1dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115b4df5c43a4fa5b85afd4ebbd16b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_fn(X_array):\n",
    "    X_df = pd.DataFrame(X_array, columns=X_esta_filtered.columns)\n",
    "    return predictor_esta.predict_proba(X_df).values  # output: array (n_samples x n_classes)\n",
    "\n",
    "background = base_datos_esta_filtered.sample(n=100, random_state=0)  # suponiendo que tienes X_train_cues\n",
    "\n",
    "# Crear el explainer con datos de fondo (background)\n",
    "explainer = shap.KernelExplainer(predict_fn, background)\n",
    "\n",
    "shap_values = explainer.shap_values(X_esta_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c6e7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "n_samples = shap_values.shape[0]\n",
    "feature_names = X_esta_filtered.columns\n",
    "\n",
    "top_features_esta = []\n",
    "\n",
    "predicted_class = predictor_esta.predict(X_esta_filtered).values[0]\n",
    "shap_vector = shap_values[0, :, predicted_class]  # (n_features,)\n",
    "# Asociar cada SHAP value con su feature\n",
    "df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap_value': shap_vector,\n",
    "    'abs_shap': np.abs(shap_vector)\n",
    "})\n",
    "\n",
    "# Seleccionar top N por magnitud\n",
    "top_10 = df.sort_values('abs_shap', ascending=False).head(top_n).reset_index(drop=True)\n",
    "\n",
    "\n",
    "top_features_esta.append({\n",
    "    'patient_index': 0,\n",
    "    'predicted_class': predicted_class,\n",
    "    'top_features': top_10[['feature', 'shap_value']]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efe70b",
   "metadata": {},
   "source": [
    "PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e85a4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_patient=df_series.loc[347]\n",
    "new_patient_psd = pd.DataFrame(new_patient).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f67188b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "\n",
    "def compute_features(signal, fs=100):  # Sampling rate updated to 100 Hz\n",
    "    if len(signal)==2048:\n",
    "        n=19\n",
    "    else:\n",
    "        n=9.5\n",
    "    # (1) Power Spectral Density (PSD) using Welch's method\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=len(signal)//n)  # Ajusta nperseg según el tamaño de la señal\n",
    "    # Filtrar entre 1 y 19 Hz\n",
    "    valid_idx = (freqs >= 1) & (freqs <= 19)\n",
    "    psd_log = np.log(psd[valid_idx])  # Escalado logarítmico\n",
    "    \n",
    "    # (2) Segmentar la señal en 4 partes\n",
    "    segment_length = len(signal) // 4\n",
    "    segments = [signal[i*segment_length:(i+1)*segment_length] for i in range(4)]\n",
    "    \n",
    "    # Calcular estadísticas por segmento\n",
    "    std_dev = [np.std(seg) for seg in segments]\n",
    "    max_amp = [np.max(np.abs(seg)) for seg in segments]\n",
    "    sum_energy = [np.sum(np.abs(seg)) for seg in segments]\n",
    "    \n",
    "    # Asegurar que solo haya 12 valores de estadísticas\n",
    "    feature_vector = np.concatenate([psd_log, std_dev, max_amp, sum_energy])\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "# Aplicar la transformación a cada celda (cada celda contiene un vector)\n",
    "def transform_dataframe(df):\n",
    "    df_transformed = df.copy()\n",
    "    for col in df.iloc[:, 1:].columns:  # Aplicar solo a columnas desde la 45 en adelante\n",
    "        df_transformed[col] = df[col].apply(lambda x: compute_features(np.array(x)))\n",
    "    return df_transformed\n",
    "\n",
    "patient_psd = transform_dataframe(new_patient_psd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5af545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6bb119f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_columns = patient_psd.columns  # omitir 'group'\n",
    "expanded_dfs_prueba = []\n",
    "\n",
    "for col in vector_columns:\n",
    "    vector = patient_psd[col].iloc[0]\n",
    "    expanded = pd.DataFrame([vector], columns=[f'{col}_vec{i}' for i in range(len(vector))])\n",
    "    expanded_dfs_prueba.append(expanded)\n",
    "\n",
    "df_expanded_psd = pd.concat(expanded_dfs_prueba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ef2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_psd_filtered = df_expanded_psd[selected_vars_psd]\n",
    "prediccion_psd = predictor_psd.predict(X_psd_filtered)\n",
    "probabilidad_psd = predictor_psd.predict_proba(X_psd_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec99d873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf0b4816eec47f38b09c5befacb1a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_datos_psd_filtered=base_datos_psd[selected_vars_psd]\n",
    "def predict_fn(X_array):\n",
    "    X_df = pd.DataFrame(X_array, columns=X_psd_filtered.columns)\n",
    "    return predictor_psd.predict_proba(X_df).values  # output: array (n_samples x n_classes)\n",
    "\n",
    "background = base_datos_psd_filtered.sample(n=100, random_state=0)  # suponiendo que tienes X_train_cues\n",
    "\n",
    "# Crear el explainer con datos de fondo (background)\n",
    "explainer = shap.KernelExplainer(predict_fn, background)\n",
    "\n",
    "shap_values = explainer.shap_values(X_psd_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "80e57546",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "n_samples = shap_values.shape[0]\n",
    "feature_names = X_psd_filtered.columns\n",
    "\n",
    "top_features_psd = []\n",
    "\n",
    "predicted_class = predictor_psd.predict(X_psd_filtered).values[0]\n",
    "shap_vector = shap_values[0, :, predicted_class]  # (n_features,)\n",
    "# Asociar cada SHAP value con su feature\n",
    "df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap_value': shap_vector,\n",
    "    'abs_shap': np.abs(shap_vector)\n",
    "})\n",
    "\n",
    "# Seleccionar top N por magnitud\n",
    "top_10 = df.sort_values('abs_shap', ascending=False).head(top_n).reset_index(drop=True)\n",
    "\n",
    "\n",
    "top_features_psd.append({\n",
    "    'patient_index': 0,\n",
    "    'predicted_class': predicted_class,\n",
    "    'top_features': top_10[['feature', 'shap_value']]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8681e89",
   "metadata": {},
   "source": [
    "STACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stack_test = np.hstack([probabilidad_psd, probabilidad_esta,probabilidad_boss,probabilidad_cues])\n",
    "y_prob_meta = meta_model.predict_proba(X_stack_test)\n",
    "y_pred_meta = np.argmax(y_prob_meta, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc2730",
   "metadata": {},
   "source": [
    "IMPORTANCIA POR MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ad493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los valores numéricos del objeto SHAP\n",
    "shap_array = shap_values.values  # shape: (422, 12, 3)\n",
    "\n",
    "# Calcular la media sobre las muestras (eje 0)\n",
    "mean_shap_per_class = np.mean(shap_array, axis=0).T  # shape: (3, 12)\n",
    "coef = mean_shap_per_class  # shape (n_classes, 6) si tienes 2 modelos y 3 clases\n",
    "\n",
    "# Sumar valor absoluto por conjunto de entrada (psd vs cues)\n",
    "psd_importance = np.mean(np.abs(coef[:, :3]), axis=0)  # columnas 0 a 2\n",
    "cues_importance = np.mean(np.abs(coef[:, 3:6]), axis=0)  # columnas 3 a 5\n",
    "esta_importance = np.mean(np.abs(coef[:, 6:9]), axis=0)  # columnas 3 a 5\n",
    "boss_importance = np.mean(np.abs(coef[:, 9:]), axis=0)  # columnas 3 a 5\n",
    "\n",
    "\n",
    "# Promedio total por fuente\n",
    "total_psd_importance = np.mean(psd_importance)\n",
    "total_cues_importance = np.mean(cues_importance)\n",
    "total_esta_importance = np.mean(esta_importance)\n",
    "total_boss_importance = np.mean(boss_importance)\n",
    "\n",
    "\n",
    "# Mostrar\n",
    "print(\"🔍 Importancia media asignada por el meta-modelo:\")\n",
    "print(f\" PSD:  {total_psd_importance:.4f}\")\n",
    "print(f\" ESTA: {total_esta_importance:.4f}\")\n",
    "print(f\" CUES: {total_cues_importance:.4f}\")\n",
    "print(f\" BOSS: {total_boss_importance:.4f}\")\n",
    "\n",
    "total_sum = total_psd_importance + total_cues_importance + total_esta_importance + total_boss_importance\n",
    "\n",
    "# Calcular porcentaje sobre 100\n",
    "psd_percent = 100 * total_psd_importance / total_sum\n",
    "cues_percent = 100 * total_cues_importance / total_sum\n",
    "esta_percent = 100 * total_esta_importance / total_sum\n",
    "boss_percent = 100 * total_boss_importance / total_sum\n",
    "\n",
    "# Mostrar\n",
    "print(\"📊 Importancia relativa por conjunto de entrada (%):\")\n",
    "print(f\" PSD:  {psd_percent:.2f}%\")\n",
    "print(f\" ESTA: {esta_percent:.2f}%\")\n",
    "print(f\" CUES: {cues_percent:.2f}%\")\n",
    "print(f\" BOSS: {boss_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Coeficientes del modelo (shape: n_clases x n_features)\n",
    "coef = mean_shap_per_class  # shape (3, 6)\n",
    "\n",
    "# Separar por bloques: PSD (col 0-2), CUES (col 3-5)\n",
    "# Calculamos importancia como valor absoluto medio por bloque, por clase\n",
    "psd_importance_per_class = np.mean(np.abs(coef[:, :3]), axis=1)   # shape: (3,)\n",
    "cues_importance_per_class = np.mean(np.abs(coef[:, 3:6]), axis=1)  # shape: (3,)\n",
    "esta_importance_per_class = np.mean(np.abs(coef[:, 6:9]), axis=1)  # shape: (3,)\n",
    "boss_importance_per_class = np.mean(np.abs(coef[:, 9:]), axis=1)  # shape: (3,)\n",
    "\n",
    "\n",
    "# Crear DataFrame para visualización\n",
    "importance_df = pd.DataFrame({\n",
    "    'Clase': [0, 1, 2],\n",
    "    'PSD_importancia': psd_importance_per_class,\n",
    "    'CUES_importancia': cues_importance_per_class,\n",
    "    'BOSS_importancia': boss_importance_per_class,\n",
    "    'ESTA_importancia': esta_importance_per_class\n",
    "\n",
    "})\n",
    "\n",
    "print(\"📊 Importancia media por clase y por fuente:\")\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mapeo del nombre del modelo a su columna en importance_df\n",
    "modelo_columna = {\n",
    "    'PSD': 'PSD_importancia',\n",
    "    'BOSS': 'BOSS_importancia',\n",
    "    'CUES': 'CUES_importancia',\n",
    "    'ESTA': 'ESTA_importancia'\n",
    "}\n",
    "\n",
    "# Diccionario con los top movements/features por modelo\n",
    "all_movements_dict = {\n",
    "    'PSD': all_movements_psd,\n",
    "    'BOSS': all_movements_boss,\n",
    "    'CUES': all_features_cues,  # <- nombre ya corregido\n",
    "    'ESTA': all_movements_esta\n",
    "}\n",
    "\n",
    "# Lista para almacenar los resultados\n",
    "ponderaciones_pacientes = []\n",
    "\n",
    "# Se asume que todas las listas tienen la misma longitud y orden\n",
    "for idx in range(len(all_movements_psd)):\n",
    "    patient_index = all_movements_psd[idx]['patient_index']\n",
    "    predicted_class = all_movements_psd[idx]['predicted_class']\n",
    "    \n",
    "    total_importance = pd.DataFrame()\n",
    "\n",
    "    for modelo, modelo_data in all_movements_dict.items():\n",
    "        entry = modelo_data[idx]\n",
    "        peso_modelo = importance_df.loc[predicted_class, modelo_columna[modelo]]\n",
    "\n",
    "        # Adaptar nombres de columna según el modelo\n",
    "        if modelo == 'CUES':\n",
    "            df = entry['all_features'].copy()\n",
    "            df.rename(columns={'feature': 'name', 'shap_value': 'shap'}, inplace=True)\n",
    "        else:\n",
    "            df = entry['all_movements'].copy()\n",
    "            df.rename(columns={'movement': 'name', 'abs_shap': 'shap'}, inplace=True)\n",
    "        \n",
    "        df['ponderada'] = df['shap'] * peso_modelo\n",
    "        df['name'] = df['name'].astype(str)\n",
    "        total_importance = pd.concat([total_importance, df[['name', 'ponderada']]])\n",
    "\n",
    "    # Agrupar por nombre (feature o movimiento) y sumar su ponderación\n",
    "    total_importance = total_importance.groupby('name', as_index=False).sum()\n",
    "\n",
    "    # Obtener el top 5\n",
    "    top5 = total_importance.sort_values(by='ponderada', ascending=False).head(5)\n",
    "    top5['patient_index'] = patient_index\n",
    "    top5['predicted_class'] = predicted_class\n",
    "\n",
    "    ponderaciones_pacientes.append(top5)\n",
    "\n",
    "# Unir todo en un único DataFrame\n",
    "final_df = pd.concat(ponderaciones_pacientes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Valor máximo global ya calculado\n",
    "max_global = final_df['ponderada'].max() + 0.001\n",
    "\n",
    "# Graficar para cada paciente\n",
    "for patient in final_df['patient_index'].unique():\n",
    "    data = final_df[final_df['patient_index'] == patient]\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(data['name'], data['ponderada'], color='orange')\n",
    "    \n",
    "    plt.title(f'Patient {patient} - Top Features/Movements (Class {data[\"predicted_class\"].iloc[0]})')\n",
    "    plt.xlabel('Feature/Movement')\n",
    "    plt.ylabel('Pondered Importance')\n",
    "    plt.ylim(0, max_global)  # Fijar eje y\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
